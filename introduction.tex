%!TEX root = main.tex
\section{Introduction}
\label{sec:intro}

In this work, we develop a \emph{bidirectional algorithm} for estimating a \emph{single element} of the product of a matrix power and a vector.
Formally, given any matrix $A\in\setR^{n\times n}$, vector $\bz$ and \emph{a target index $t\in\{1,2,\ldots,n\}$}, we consider the problem of computing $\pzlt := \ip{A^{\ell}\bz}{\be_t}$ for given exponent $\ell\in\mathbb{N}$.

Computing $A^{\ell}\bz$ is a basic problem in matrix computations, and there is a large body of work on efficient ways of performing exact computation. 
However, in large-scale settings (i.e., with very large values of $n$), direct computation is often infeasible, and one needs to resort to estimating $A^{\ell}\bz$.
Moreover, we are interested in developing techniques which can estimate a single element of $A^{\ell}\bz$ in running time which is provably faster than computing the entire vector. 

The problem of estimating $\pzlt$ has gained attention recently in the context of estimating random-walk transition probabilities, in particular, for PageRank and Personalized PageRank~\cite{Page1999}, and other network centrality metrics. 
%Though PageRank had fast estimation algorithms based on iterative~\cite{Andersen2007} and MCMC~\cite{Avrachenkov2007} techniques, these 
A recent line of work~\cite{Lofgren2014,lofgren2016personalized,banerjee2015fast} has shown how to develop fast bidirectional algorithms for this problem, and more generally, for settings where $A^T$ is a \emph{stochastic} matrix and $\bz$ is a probability distribution (i.e., an element of the $n$-dimensional simplex).
Our work generalizes these techniques to more general $A,\bz$; in particular, our main result can be paraphrased as follows:
\begin{proposition}
Given matrix $A$ (with $||A||_1\leq 1$), vector $z$ (with $||z||_1\leq 1$) and any index $t$ in $[n]$, Algorithm \texttt{BIDIR-MATRIX-POWER} (cf. Section \ref{ssec:bidiralgo}) returns estimate $\pzltest$ such that with high probability $|\pzlt-\pzltest|<\max\{\epsilon\pzlt,1/n\}$, with average running time of $\widetilde{O}\left(\left(nnz(A)/\epsilon\right)^{2/3}\right)$ for uniform random choice of $t$.
\end{proposition}	

This result is interesting in that it generalizes the existing results for stochastic matrices to arbitrary matrices, albeit with a loss in the running time scaling~\footnote{For estimating $\ell$-step transition probabilities, the running time of an equivalent algorithm in ~\cite{banerjee2015fast} is $\widetilde{O}\left(\left(nnz(A)/\epsilon\right)^{1/2}\right)$ for uniform random choice of $t$.}. 
Moreover, computing $\left(A^{\ell}\bz\right)[t]$ is often a subroutine in more complex algorithms for various applications.
In Section \ref{sec:linsolve}, we focus on one such application -- the von Neumann-Ulam scheme for solving linear equations~\cite{forsythe1950matrix}. 
This has been gaining interest as a promising candidate for solving large-scale equations, owing to its ease of parallelization and asynchronous and local nature~\cite{ji2013convergence,dimov2015new,lee2014asynchronous}. 
More generally however, this algorithm may prove useful in more complex estimation tasks which use matrix polynomials as approximators. 


\subsection{Related Work}

Previous work has addressed both the problem of estimating a single component of a matrix equation and the problem of computing matrix inversions through Monte Carlo methods. The von Neumann-Ulam algorithm shows that the inverse of $B$ by defining $A = I - B$ and running random walks over the induced graph of $A$ \cite{forsythe1950matrix}. As long as the spectral norm of $A$ is less than $1$, the expectation of these random walks is exactly $(A)_{ij}$ where $i$ is the start node and $j$ is the end node. With these methods, one can solve the system $\mathbf{x} = G\mathbf{x} + \mathbf{z}$ provided the spectral norm $\rho(G) < 1$ since $\mathbf{x} = (I-G)^{-1}\mathbf{z}$, exactly the problem the von Neumann-Ulam algorithm solves. \cite{ji2013convergence} proved that there exist matrices $G$ satisfying $\rho(G) < 1$ but with $||G||_\infty > 1$, and that convergence of the von Neumann-Ulam algorithm is not guaranteed on this class of matrices.  Wu and Gleich address this issue by proposing a new algorithm that will converge as long as $\rho(G^+) < 1$, a weaker condition than $\rho(G) < 1$ \cite{wu2016multi}.

We can also view the problem of estimating a single component $\mathbf{x}[t]$ as computing the contributions in reverse from a source distribution to $\mathbf{x}[t]$.This problem was originally addressed for the special case of computing Personalized Page Rank (PPR) in \cite{andersen2007local} via the Reverse-Push algorithm, which pushes back the unit contribution using a series of reverse operations. \cite{andersen2007local} shows that this problem for the specific case of PPR can return an $\epsilon$ accurate estimate of $ppr(v)$ assuming $ppr(v) \geq \alpha$ in time $O\left( \frac{1}{\alpha \epsilon}\right)$.
This approach also resembles the work in \cite{lee2014asynchronous}, which describes a similar algorithm using an asynchronous procedure. \cite{lee2014asynchronous} performs a series of local updates using a residual vector, and show that with this procedure one can estimate $\mathbf{x}[t]$ satisfying $|\mathbf{x}[t] - \hat{\mathbf{x}[t]}| \leq \epsilon ||\mathbf{x}||$ in time $O\left(\min\{ \epsilon^{\ln d /\ln (|| G||_2)}, n \ln \epsilon / \ln (||G||_2)\} \right)$. 

Our work is based on recent developments in bidirectional algorithms for estimating single-state transition probabilities in Markov chains.
Such algorithms were first developed for \emph{reversible Markov chains} using random-walk collision statistics; in particular, Kale et al.~\cite{Kale2008} proposed such a technique for estimating length-$2\ell$ random walk transition probabilities in a \emph{regular undirected graph}.
The main idea is that to test if a random walk goes from $s$ to $t$ in $2\ell$ steps with probability $\geq\delta$, we can generate two independent random walks of length $\ell$, starting from $s$ and $t$ respectively, and detect if they collide, i.e., terminate at the same intermediate node. 
The critical observation is that using $\sqrt{n}$ walks from $s$ and $t$ gives $n$ potential collisions, which is sufficient to estimate probabilities on the order of $1/n$.
This argument draws from older ideas on using the \emph{birthday-paradox} in estimation problems~\cite{Motwani2007}.
A bidirectional algorithm for general graphs was first developed by Lofgren et al.~\cite{Lofgren2014} for PageRank estimation; the argument was subsequently simplified in \cite{lofgren2016personalized}, and extended to general Markov chains in \cite{banerjee2015fast}. 
Our work here further generalizes this line of work to computing single elements of powers of arbitrary matrices, with a particular application to solving for single elements in sparse linear systems.


\section{Solving Linear Equations via Series Approximation}
\label{sec:linsolve}

Unless specified otherwise, we use boldface letters (e.g. $\mathbf{x},\mathbf{y}$) to denote vectors in $\setR^{n\times 1}$, and capital letters (e.g. $A,Q$) to denote matrices in $\setR^{n\times n}$.


Given a linear system $\mathbf{y} = A\mathbf{x}$, where $A$ is a positive definite matrix, our aim is to estimate $\mathbf{x}[t]$ for some given index $t\in[n]$. 
This can be done by directly solving for $\mathbf{x}$; however, we are interested in settings where $n$ is very large, and hence direct solution techniques may be impractical. 

One approach for approximating the solution to $\mathbf{y} = A\mathbf{x}$ is to expand it via the Neumann series and then compute the leading terms of the summation.
In particular, if $A$ is positive definite, \nsedit{we can find $\gamma$ such that $G = I - \gamma A$ satisfies $\rho(G) < 1$. Then $\gamma\mathbf{y} = (I - (I - \gamma A)\mathbf{x}$  and we have a new system $\mathbf{x} = G\mathbf{x} + \mathbf{z}$ where $\mathbf{z} = \gamma \mathbf{y}$ and $G = I - \gamma A$. }
\nsedit{Since we ensure $\rho(G)<1$, we can write $\mathbf{x}$ as a von Neumann series: $\mathbf{x} = \sum_{k=0}^\infty G^k \mathbf{z}$.}
Thus to find the $t$ component of the solution vector $\mathbf{x}$, or $\mathbf{x}[t]$, we perform the operation $\ip{\mathbf{x}}{\mathbf{e}_t} = \sum_{k=0}^\infty \ip{G^k \mathbf{z}}{\mathbf{e}_t}$. 
Similar transformations have been used in \cite{dimov2015new, lee2014asynchronous, wu2016multi}.

\nsedit{Computing $\mathbf{x}[t]$ then amounts to computing $\pzlt := \ip{G^{\ell}\mathbf{z}}{\mathbf{e}_t} = \ip{\mathbf{z}}{(G^T)^{\ell}\mathbf{e}_t}$ for any $\ell$ and taking their sum for some $\ell \in \{0,\ldots,\lm\}$ where $\lm$ is a finite term truncating the power series.}
Let $Q:=G^T$; prior work by Banerjee and Lofgren \cite{banerjee2015fast} shows how to compute $\pzlt$ for the special case where $\mathbf{z}$ is a probability vector and $Q$ is a \emph{stochastic matrix} (i.e., with all nonnegative entries, and each row summing to $1$). 
We now extend this result for any $\mathbf{z}$ and a special class of matrices $Q$.

\nsedit{
Note that the error from truncating the series to $\lm$ can be determined a priori; using bounds for $\mathbf{z}$ and the condition that $G$ has spectral norm less than $1$, we can set $\lm \geq \frac{1}{\ln \rho(G)} \ln \left(\frac{\Delta(1 - \rho(G))}{||\mathbf{z}||} \right)$  to bound the series truncation error by $\Delta$.
\begin{proof}
Let $\varepsilon$ be the error from truncating the power series to $\lm$. Then by definition:
\begin{align*}
\varepsilon &= \left|\sum_{\ell=0}^\infty \ip{\mathbf{z}}{Q^\ell\mathbf{e}_t} - \sum_{\ell=0}^{\lm} \ip{\mathbf{z}}{Q^\ell\mathbf{e}_t} \right| \\
%&=\left| \sum_{\ell= \lm+1}^\infty \ip{\mathbf{z}}{Q^\ell\mathbf{e}_t} \right|\\
&= \left| \ip{\mathbf{z}}{ Q^{\lm + 1} \sum_{\ell= 0}^\infty Q^\ell\mathbf{e}_t} \right|\\
\end{align*} 
For any inner product of the form $\ip{\mathbf{z}}{Q^\ell \mathbf{e}_t}$, by the Cauchy - Schwarz inequality we have $|\ip{\mathbf{z}}{Q^\ell \mathbf{e}_t}| \leq ||\mathbf{z}|| \cdot ||{Q}^\ell \mathbf{e}_t|| \leq ||\mathbf{z}|| \rho(Q)^{\ell}$.
Hence we have
$\varepsilon \leq %||\mathbf{z}|| \rho(Q)^{\lm+1}\left( \sum_{\ell = 0}^{\infty}{\rho(Q)^{\ell}} \right) \\
 ||\mathbf{z}|| \rho(Q)^{\lm+1}\left(\frac{1}{1 - \rho(Q)} \right)$
Now suppose we want our error $\epsilon \leq \Delta$. Then we can set the upper bound for $\epsilon$ to $\Delta$ and solve for $\lm$.
%\begin{align*}
%||\mathbf{z}|| \rho(Q)^{\lm+1}\left(\frac{1}{1 - \rho(Q)} \right) =  \Delta \\
%(\lm + 1)\ln \rho(Q) = \ln \left(\frac{\Delta(1 - \rho(Q)) }{||\mathbf{z}||}\right) \\
%$\lm = \frac{1}{\ln \rho(Q)}  \ln \left(\frac{\Delta(1 - \rho(Q)) }{||\mathbf{z}||}\right) - 1$
Thus, if we take $\lm \geq \frac{1}{\ln \rho(Q)}  \ln \left(\frac{\Delta(1 - \rho(Q)) }{||\mathbf{z}||}\right)$, we will have an error of at most $\Delta$ when approximating the series.
\end{proof}
Since we can a priori bound the error resulting from truncating the von Neumann series to $\lm$, we will focus on the problem of estimating $\pzlt$. It is then straightforward to develop overall error bounds by combining the results of the additive truncation error $\Delta$ and the relative error guarantees we prove for our bidirectional algorithm in Theorem 1. 
}
