%!TEX root = /Users/siddhartha/Repos/Fast-Linsolve/main.tex
\section{Existing Algorithms for Estimating Matrix Powers}

We now describe our algorithm for computing an estimate $\pzltest$ for $\pzlt =\ip{\bz}{Q^{\ell}\mathbf{e}_t}$.
To do so, we first describe two existing algorithms -- a forward MCMC technique based on the von Neumann-Ulam scheme~\cite{Wasow1952,ji2013convergence}, and a variational method based on a natural local dynamic-programming update, proposed by Andersen et al.~\cite{andersen2007local} for computing PageRank, and used by Lee et al.~\cite{lee2014asynchronous} in this setting. 
We present these along with statements of their correctness and running time -- some of these results follow directly from previous work (as we note in the appropriate sections), and their proofs are included here mainly for the sake of completeness.


Our main contribution in this work is to show how the above two primitives can be combined into a bidirectional algorithm for estimating $\pzltest$ for any given matrix $Q$. 
Our algorithm follows the general structure proposed by Lofgren et al.~\cite{Lofgren2014,banerjee2015fast} for PageRank and Markov Chain transition probability estimation.
It comprises of two distinct components: first we use the reverse local-DP primitive to estimate approximate values of $\left(Q^{\ell}\be_t\right)[i]$ for all steps $\ell \in[\lm]$ and $i\in[n]$. 
We then use MCMC samples to reduce the error in these estimates to get our desired accuracy.


We first introduce some notation which will help us better describe the algorithm.
Drawing parallels to the case where $Q$ is a stochastic matrix and $\bz$ an element in the $n$-dimensional simplex (as in~\cite{banerjee2015fast}), we define a (weighted) directed graph $\G_Q(\V,\E)$ with states $\V = [n]$, and edges $(i,j)\in \E$ if $Q_{ij}\neq 0$. 
Each edge $(i,j)\in\E$ has an associated weight $w_{ij}\in\setR$, which we describe later.
We refer to the label for a node $v\in V$ (i.e., a dimension $v\in[n]$) as a \emph{dimension-index}, and the exponent of $Q$ as the \emph{step-index}.
We also use $\mathbf{e}_v$ denote the indicator for index $v$ (i.e., $\mathbf{e}_v[i] = \mathds{1}_{i=v}$). 
Finally, we define $Q^+$ to denote the matrix with $Q^+_{ij} = |Q_{ij}|$. 
Many of our bounds depend on the norm $||Q^+||_{\infty} = ||Q||_{\infty}$, i.e., the maximum (absolute) row-sum of $Q$; for ease of notation, we henceforth define $\beta = ||Q||_{\infty}$. 

%Finally, we we denote $\beta = ||Q||_\infty$ to streamline our proofs, since $\beta$ encodes whether a matrix is substochastic, or must be normalized to become stochastic.


\subsection{Computing Matrix Powers via Iterative Local-Update}
\label{ssec:reverse}

One approach towards estimating $\pzlt$ is via a standard power iteration for computing $Q^{\ell}\be_t$. 
In settings where $n$ is large enough such that a direct power iteration is infeasible, one can use a `local' power iteration, which essentially corresponds to a natural dynamic programming update. Informally, the algorithm estimates $\pzlt$ by starting off with a mass of $1$ on dimension-index $t$, and then `pushing' this mass in reverse along the edges of graph $G_Q$. 

To describe this \texttt{REVERSE-LOCAL-UPDATE} algorithm, we first define a \texttt{REVERSE-PUSH} operation corresponding to the DP iteration. 
This is directly adapted from the algorithm in Andersen \cite{andersen2007local} for the personal PageRank problem (and more generally for a stochastic matrix $Q$); however, it is straightforward to show that the invariant that holds for any matrix $Q$ (in fact, it does not require $Q$ to be full rank, as we have assumed).
 
The \texttt{REVERSE-PUSH} operation is a standard dynamic programming iteration which is used to compute an estimate $\pzltest$ by proceeding `in reverse' from $t$. 
Essentially,\texttt{REVERSE-PUSH} is a local power-iteration for computing $Q^{\ell}\mathbf{e}_t$ ; instead of performing a full power-iteration, it adaptively exploits any sparsity in the computation.
This operation was defined in the form given below in \cite{andersen2007local}, and subsequently used as a primitive in \cite{banerjee2015fast, lee2014asynchronous}.


For each step-index $\ell\in\{0,1,\ldots,\lm\}$, we store two vectors: the \emph{estimate vector} $\qest{\ell}$ and the \emph{residual vector} $\res{\ell}$.
We initialize all $\res{\ell},\qest{\ell}, \ell\in[\lm]$ to $0$, except for $\res{0}$, which we set to $\mathbf{e}_t$.
Now, given any dimension-index $v\in[n]$ and step-index $\ell\in[\lm]$, the \texttt{REVERSE-PUSH} operation iteratively updates these vectors as follows:
\begin{algorithm}[!ht]
\caption{\texttt{REVERSE-PUSH}$(t,v,\ell)$}
\label{alg:push}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, estimates $\qest{\ell}$, residuals $\res{\ell},\res{\ell+1}$
\RETURN New estimates $\qestnew{\ell}$ and residuals $\resnew{\ell}$ computed as:
\begin{align*}
	\qestnew{\ell} &\leftarrow \qest{\ell} + \ip{\res{\ell}}{\mathbf{e}_v}\mathbf{e}_v \\
	\resnew{\ell} &\leftarrow \res{\ell} - \ip{\res{\ell}}{\mathbf{e}_v}\mathbf{e}_v \\
	\resnew{\ell+1} &\leftarrow \res{\ell+1} + \ip{\res{\ell}}{\mathbf{e}_v}(Q\mathbf{e}_v)
\end{align*}	
\end{algorithmic}
\end{algorithm}    

The \texttt{REVERSE-PUSH} iteration results in the following critical invariant for the estimate and residual vectors:
\begin{lemma}
\label{lem:pushinvariant}
Given the initialization described above, after any sequence of \texttt{REVERSE-PUSH} operations, and for any $\bz\in\setR^n$ and $\ell\geq 0$, the estimates $\{\mathbf{q}_t^k\}$ and residuals $\{\mathbf{r}_t^k\}$  satisfy the following invariant:
\begin{align*}
\mathbf{p}_\bz^{\ell}[t] &= \left< \bz, \mathbf{q}_t^{\ell} \right> + \sum_{k=0}^{\ell} \left<\bz, Q^k\mathbf{r}_t^{\ell-k}\right> = \left< \bz, \mathbf{q}_t^{\ell}  + \sum_{k=0}^{\ell} Q^k\mathbf{r}_t^{\ell-k}\right>
\end{align*}
\end{lemma}

The above invariant was first stated in~\cite{andersen2007local} for the case of PageRank vectors. 
For the sake of completeness, we present a proof below for general matrices, adapted from~\cite{Lofgren2014}; an identical invariant is given in~\cite{lee2014asynchronous}.

\begin{proof}
For our chosen initialization (i.e., $\res{0} = \mathbf{e}_t$, and all other estimate and residual vectors set to $0$), the invariant simplifies to $\mathbf{p}_\bz^{\ell}[t] = \ip{\bz }{Q^{\ell}\mathbf{e}_t}$ which is true by definition.
Now, assuming the invariant holds at any stage with vectors $\{\qest{\ell}\}, \{\res{\ell}\}_{\ell\in[\lm]}$, and let $\{\qestnew{\ell}\}, \{\resnew{\ell}\}_{\ell\in[\lm]}$ be the new vectors after executing a \texttt{REVERSE-PUSH}$(t,v,k)$ operation for any given $k\in[\lm]$ and $\bz\in\setR^n$. We define:
$$\Delta_v^{k} = \left( \mathbf{\tilde{q}}_t^{\ell} + \sum_{i=0}^{\ell}  (Q^i)\resnew{\ell-i} \right)- \left( \qest{\ell} + \sum_{i=0}^{\ell}  (Q^i) \mathbf{r}_t^{\ell-i}\right)$$
Now to show that the invariant holds following \texttt{REVERSE-PUSH}$(t,v,k)$, it suffices to show that $\Delta_v^{k}$ is zero for any $v\in V$ and $k\in[\lm]$. 

We now have three cases: $(i)$ if $\ell < k$, then the \texttt{REVERSE-PUSH}$(t,v,k)$ operation does not affect the residual or estimate vectors $\{\mathbf{q}_t^{i},\mathbf{r}_t^{i}\}_{i<k}$, and hence 
$\Delta_v^k=0$;
$(ii)$ If $\ell = k$, we have:
\begin{align*}
\Delta_v^{k} &= \left( \mathbf{\tilde{q}}_t^{k} +  \mathbf{\tilde{r}}_t^{k}\right)- \left( \mathbf{q}_t^{k} + \mathbf{r}_t^{k}\right)\\
&= \mathbf{q}_t^{k} + \left<\mathbf{r}_t^{k}, \mathbf{e}_v \right>\mathbf{e}_v +  \mathbf{r}_t^{k} - \left<\mathbf{r}_t^{k}, \mathbf{e}_v \right>\mathbf{e}_v - \mathbf{q}_t^{k} - \mathbf{r}_t^{k}
= 0
\end{align*}
$(iii)$ Finally, when $\ell > k$, we have: 
\begin{align*}
\Delta_v^k &= Q^{\ell-k}\left( \mathbf{\tilde{r}}_t^k - \mathbf{r}_t^k \right) + Q^{\ell-k-1}\left( \mathbf{\tilde{r}}_t^{k+1} - \mathbf{r}_t^{k+1} \right)\\
&= -\left<\mathbf{r}_t^k, \mathbf{e}_v \right>Q^{\ell-k}\mathbf{e}_v  + \left<\mathbf{r}_t^k, \mathbf{e}_v \right>Q^{\ell-k-1}\left(Q\mathbf{e}_v \right)
%&= -\left<\mathbf{r}_t^k, \mathbf{e}_v \right>\mathbf{e}_v(Q^T)^{\ell-k} + \left<\mathbf{r}_t^k, \mathbf{e}_v \right>\mathbf{e}_v (Q^T)^{\ell-k} 
= 0
\end{align*}
Hence we have shown that the invariant is preserved for any sequence of reverse push operations.
\end{proof}

The above invariant gives a natural iterative algorithm for computing $\pzlt$, by performing repeated \texttt{REVERSE-PUSH} operations and controlling the residual vectors $\mathbf{r}_t^k$, and using $\pzltest = \langle\bz,\mathbf{q}_t^{\ell}\rangle$ as the estimate. 
Depending on the norm we choose to control, we can get a bound for the error via H{\"o}lder's inequality.
In particular, controlling the infinity norm (i.e., the maximum absolute value of the residual vectors) to be less than some chosen $\delta_r>0$ gives us a bound: $|\pzlt - \langle\bz,\mathbf{q}_t^{\ell}\rangle|\leq ||x||_1\delta_r\beta^{\ell}$~\footnote{This approach was used in~\cite{andersen2007local,Lofgren2014,banerjee2015fast}; an alternative is to control $||r_t^k||_2$ giving error bounds in terms of $||x||_2$, which was suggested in ~\cite{lee2014asynchronous}.}.

\begin{algorithm}[ht]
\caption{\texttt{REVERSE-LOCAL-UPDATE}$(t,Q, \lm, \delta_r)$}
\label{alg:rwork}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, maximum step-index $\lm$, target residual threshold $\delta_r$
\STATE{Initialize all residual $\res{\ell}$ and estimate vectors $\qest{\ell}, \ell\in[\lm]$ to $0$; set $\res{0} = \mathbf{e}_t$}
\FOR{ $\ell \in \{0, 1, 2, ... \lm\}$ }
\WHILE{$\exists v$ such that $\left|\res{\ell}[v]\right| > \delta_r $}
\STATE{\texttt{REVERSE-PUSH}$(t,v,\ell)$}
\ENDWHILE
\ENDFOR
\RETURN $\{\qest{\ell}\}, \{\res{\ell}\}_{\ell\in[\lm]}$
\end{algorithmic}
\end{algorithm}    

Finally, we want to bound the running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\lm,\delta_r)$. 
It is easy to see that in the worst case, the running time can be as much as the $\lm$-hop in-neighborhood of $t$ in $Q$. 
However, for a \emph{uniform random} choice of $t$, we can obtain a more informative bound. 
Recall we define $\beta = ||Q||_{\infty}$. Now we have the following:

\begin{lemma}
\label{lem:pushinvariant}
For any $Q\in\setR^{n\times n}$ and uniform random dimension-index $t\in[n]$, the expected running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\lm,\delta_r)$ is 
$$O\left(\frac{nnz(Q)}{n\delta_r}(\lm+1)\beta^{\lm}\right)$$ 
\end{lemma}

In particular, note that if $||Q||_{\infty}\leq 1$ and $\lm=O(1)$, then the average running time is $O\left(\frac{nnz(Q)}{n\delta_r}\right)$.

\begin{proof}
Let $T(t)$ be the running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\lm,\delta_r)$.
Recall we define $Q^+$ as the matrix with $Q^+_{ij} = |Q_{ij}|$; let $\hat{T}(t)$ be the running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q^+,\lm,\delta_r)$. 
Then we have that for every matrix $Q$ and every $t$, we have $\hat{T}(t)>T(t)$ -- this follows from the fact that any cancellation between positive and negative residuals in \texttt{REVERSE-LOCAL-UPDATE}$(t,Q^+,\lm,\delta_r)$ can only decrease the number of iterations. 
Also, note that under $Q^+$, since all residuals are positive, we have that for any $\ell\leq\lm,v\in[n]$, the residuals satisfy $\mathbf{r}_t^{\ell}[v]\leq \left(\mathbf{e}_v^TQ^\ell\right)[t]$.

Now let $d_i := \sum_j\mathds{1}_{\{Q_{ij}\neq 0\}}$, i.e., the support of $i^{th}$ row in $Q$, and $\mathbf{r}_t^{\ell}$ denote the residuals under \texttt{REVERSE-LOCAL-UPDATE}$(t,Q^+,\lm,\delta_r)$. 
%Recall we define $\beta = \max\{1,||Q||_{\infty}\}$.
Recall we define $\beta = ||Q||_{\infty}$.From Algorithm \ref{alg:rwork}, we have $\hat{T}(t) = \sum_{\ell=0}^{\lm}\sum_{v\in[n]}\mathds{1}_{\mathbf{r}_t^{\ell}[v]>\delta_r}$.
Thus, the expected running time over a uniform random choice of $t\in[n]$ is given by
\begin{align*}
\frac{1}{n}\sum_t\hat{T}(t) &= \frac{1}{n}\sum_{t\in[n]}\sum_{\ell=0}^{\lm}\sum_{v\in[n]}\mathds{1}_{\{\mathbf{r}_t^{\ell}>\delta_r\}}d_v\\
&=\frac{1}{n}\sum_{\ell=0}^{\lm}\sum_{v\in[n]}\sum_{t\in[n]}\mathds{1}_{\{\mathbf{r}_t^{\ell}>\delta_r\}}d_v\\
&\leq\frac{1}{n}\sum_{\ell=0}^{\lm}\sum_{v\in[n]} \mathds{1}_{\{\left(\mathbf{e}_w^T(Q^+)^\ell\right)[t]>\delta_r\}}d_v\\
&=\frac{1}{n}\sum_{\ell=0}^{\lm}\sum_{v\in[n]} \frac{||\mathbf{e}_w^T(Q^+)^\ell||_1}{\delta_r}d_v\\
&\leq\frac{1}{n}\sum_{\ell=0}^{\lm}\sum_{v\in[n]} \frac{||Q^+||_{\infty}^\ell}{\delta_r}d_v\\
&\leq\left(\lm+1\right)\beta^{\lm} \frac{nnz(Q)}{n\delta_r}
\end{align*}
\end{proof}


\subsection{Computing Matrix Powers via MCMC Sampling}
\label{ssec:forwardwork}


In the previous section, we computer $\pzlt$ by working backwards from $t$. 
Note that our final algorithm is independent of $\bz$.
We now present an alternate technique which is based on a forward MCMC sampling technique called the von Neumann-Ulam scheme. 
Note that in this case, the algorithm starts from $\bz$, and computes $\pzlt$ for all $t\in[n]$.

More generally, given any vectors $\mathbf{a}$ and $\mathbf{b}$ and matrix $Q$, the von Neumann-Ulam scheme can be used for computing $\ip{\mathbf{a}}{Q^k\mathbf{b}}$.
To understand the algorithm, note that we can expand $\ip{\mathbf{a}}{Q^k\mathbf{b}}$ as the sum $\sum_{(v_0,\ldots, v_k) \in V^k}\left(\prod_{j \in [k]} Q_{v_{j-1}v_{j}} \right) \mathbf{a}[v_0] \mathbf{b}[v_k]$. 
Now we can interpret this sum as an expectation over a $k$-step random walk $W = (V_0,V_1,\ldots,V_k)$ on $\G$, specified as follows:
\begin{itemize}
\item $V_0$, the starting node for the random walk, is sampled from $\sigma_{\mathbf{a}} = \{|\mathbf{a}[i]|/||\mathbf{a}||_1\}_{i\in [n]}$, and has an associated weight $w_{V_0} = sign(\mathbf{a}[V_0])||\mathbf{a}||_1$.
\item The transition probability matrix for the walk is given by $P = \{|Q_{ij}|/||Q_i||_1\}$ (where $||Q_i||_1$ is the $1$-norm of the $i^{th}$ row of $Q$).
\item Each edge $(i,j)\in E$ has associated weight $w_{ij} = sgn(Q_{ij})||Q_i||_1$. 
\item The `score' for a walk $W$ is the product of weights of traversed edges, i.e.,
$$S_t^k(W) = w_{V_0}\prod_{i=0}^{k-1}w_{V_iV_{i+1}}\mathbf{b}[V_{k}]$$
\end{itemize}
%We can then sample paths according to $P$ and weight each of these products by the normalization factors we used to construct $P$ and $\sigma$ from $Q$ and $\bz$. There are two aspects to normalizing $Q$ and $\bz$, the sign, and ensuring row sums are equal to $1$. By normalizing the substochastic matrix $Q$ to get the stochastic $P$, we are removing a dummy state $A$ that absorbed probability for $Q$. 
%We construct $\mathbf{p}_A$ to store this leftover probability mass. 
%We sample from the distribution $\sigma (P)^k$ and then weight by the normalization factors to compute $\sum_{(v_0,\ldots, v_k) \in V^k}\left(\prod_{j \in [k]} Q_{v_{j-1}v_{j}} \right) \bz[v_0] \mathbf{r}_t^{\ell-k}[v_k]$ as an expectation. 
This procedure is summarized in Algorithm \ref{alg:fwalk}. 

\begin{algorithm}[ht]
\caption{\texttt{MCMC-SAMPLER}$(Q, k, \mathbf{a}, \mathbf{b})$}
\label{alg:fwalk}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, exponent $k$, vectors $\mathbf{a}$ and $\mathbf{b}$
\STATE{Construct transition matrix $P = \{|Q_{ij}|/||Q_i||_1\}$ and starting measure $\sigma_a = \{|\mathbf{a}[i]|/||\mathbf{a}||_1\}_{i\in V}$}
\STATE{Define node weights $w_{V_0} = sign(\mathbf{a}[V_0])||\mathbf{a}||_1$ and edge weights $w_{V^iV^j} = sign(Q_{V^iV^j})||Q_i||_1$}
\STATE{Construct source distribution $\sigma_{\mathbf{a}}$ with $\sigma_{\mathbf{a}}[i] = \frac{\bz[i]}{||\bz||_1}$}
\STATE{Sample $V^0 \sim \sigma_a$, and generate a random walk $W = 	\{V^0, V^1,\ldots, V^{k}\}$ of length $k$ on $\G$  using transition probability $P$.}
\RETURN Walk-score $S_{t}^k(W) = w_{V_0}\prod_{i=0}^{k-1}w_{V_iV_{i+1}}\mathbf{b}[V_{k}]$
\end{algorithmic}
\end{algorithm} 

%Once we have the sampled scores for $n_f$ walks each of length $l$, we can combine the estimate $\ip{\bz}{\qest{\ell}}$ with an average over the scores $\{S_{t, i}^{\ell}\}$ from to compute the overall estimator $\pzltest$.
%Again, as noted in the section describing the forward walk estimation, provided that $Q$ is substochastic with spectral norm less than $1$, we can find an induced transition matrix that is fully stochastic as well as the described normalization terms $\pabsorb{\ell}$.

Recall we define $\beta = \max\{1,||Q||_{\infty}\}$. 
Now we have the following Lemma.
\begin{lemma}
\texttt{MCMC-SAMPLER}$(Q, k,\mathbf{a},\mathbf{b})$ returns a walk-score $S_{t}^k(W)$ which satisfies:
\begin{enumerate}
\item $\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^k} \left[S_{t}^k(W)\right] = \left<\mathbf{a}, Q^k\mathbf{b} \right>$
\item $S_{t}^{k}(W) \in [\;-\beta^k||\mathbf{a}||_1||\mathbf{b}||_{\infty} ,\; \beta^k||\mathbf{a}||_1||\mathbf{b}||_{\infty} \;]$
\end{enumerate}
\end{lemma}


\begin{proof}
We can expand $\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^k} \left[S_{t}^k(W)\right]$ to obtain:
\small
\begin{align*}
&\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^k} \left[S_{t}^k(W)\right] \\ 
&= \sum_{(V^0, ... V^{k}) \in [n]^{k+1}} \left(\frac{sign(\mathbf{a}[V_0])||\mathbf{a}||_1|\mathbf{a}[V^0]|}{||\mathbf{a}||_1} \right. \\
&\left. \times \left( \prod_{j \in [0, k-1]} \frac{sign(Q_{V^jV^{j+1}}||Q_{V^j}||_1)|Q_{V^jV^{j+1}}|}{||Q_{V^j}||_1} \right) \mathbf{b}[V^k] \right) \\
& = \sum_{(V^0, ... V^{k}) \in [n]^{k+1}} \left(\mathbf{a}[V_0] \left( \prod_{j \in [0, k-1]} Q_{V^jV^{j+1}}\right) \mathbf{b}[V^k] \right)
\end{align*}
\normalsize
The final expression is exactly the definition of $\ip{\mathbf{a}}{Q^k\mathbf{b}}$; hence $\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^k} \left[S_{t}^k(W)\right] = \ip{\mathbf{a}}{Q^k\mathbf{b}}$.


Next, in order to bound the score $S_{t}^k(W)$, recall that
\begin{align*}
S_{t}^k(W) = w_{V_0}\prod_{i=0}^{k-1}w_{V_iV_{i+1}}\mathbf{b}[V_{k}]
\end{align*}

We defined $w_{V_0} = sign(\mathbf{a}[V_0])||\mathbf{a}||_1$, so trivially $|w_{V_0}| \leq ||\mathbf{a}||_1$.
Additionally, $w_{V_iV_{i+1}} = sign(Q_{V^iV^j})||Q_i||_1 $ and by definition $||Q_i||_1  \leq ||Q||_{\infty}$ and $| \mathbf{b}[V^k] |\leq ||\mathbf{b}||_\infty$.
Hence, since we are multiplying $k$ edge scores, we finally obtain
$|S_{t}^k(W)| \leq ||Q||_{\infty}^k||\mathbf{a}||_1||\mathbf{b}||_\infty$ as stated in the lemma.
\end{proof}

