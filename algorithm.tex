%!TEX root = main.tex
\section{Existing Algorithms for Estimating Matrix Powers}
\label{sec:existing}

Based on the above discussion, we henceforth focus on developing bidirectional algorithms for estimating $\pzlt =\ip{\bz}{Q^{\ell}\mathbf{e}_t}$.
To do so, we first describe two existing algorithms that we use as primitives for our procedure: a forward MCMC technique based on the von Neumann-Ulam scheme~\cite{Wasow1952,ji2013convergence}, and a local variational method proposed by Andersen et al.~\cite{andersen2007local} for computing PageRank, and used by Lee et al.~\cite{lee2014asynchronous} in this setting. 
We present these along with their accuracy and running time guarantees -- some of these results follow directly from previous work (as we note in the appropriate sections), and we include their proofs here mainly for the sake of completeness.

We first introduce some notation which will help us better visualize the algorithm.
Drawing parallels to the case where $Q$ is a stochastic matrix and $\bz$ an element in the $n$-dimensional simplex (as in~\cite{banerjee2015fast}), we define a (weighted) directed graph $\G_Q(\V,\E)$ with states $\V = [n]$, and edges $(i,j)\in \E$ if $Q_{ij}\neq 0$. 
Each edge $(i,j)\in\E$ has an associated weight $w_{ij}\in\setR$, which we describe later.
We refer to the label for a node $v\in V$ (i.e., a dimension $v\in[n]$) as a \emph{dimension-index}, and the exponent of $Q$ as the \emph{step-index}.
Finally, we define $Q^+$ to denote the matrix with $Q^+_{ij} = |Q_{ij}|$. 
%Many of our bounds depend on the norm $||Q^+||_{\infty} = ||Q||_{\infty}$, i.e., the maximum (absolute) row-sum of $Q$
%; for ease of notation, we use the shorthand $\beta := ||Q||_{\infty}$. 


\subsection{Computing Matrix Powers via Iterative Local-Update}
\label{ssec:reverse}

One approach for estimating $\pzlt$ is via a standard power iteration for computing $Q^{\ell}\be_t$. 
In settings where $n$ is large such that direct power iteration is infeasible, one can use a `local' power iteration, which corresponds to a natural dynamic programming update. Informally, the algorithm estimates $\pzlt$ by starting off with a mass of $1$ on dimension-index $t$, and then `pushing' this mass in reverse along the edges of graph $G_Q$. 

To describe this \texttt{REVERSE-LOCAL-UPDATE} algorithm, we first define a \texttt{REVERSE-PUSH} local iteration. 
Essentially, this is a local power-iteration for computing $Q^{\ell}\mathbf{e}_t$, which adaptively exploits any sparsity in the computation.
This operation was defined by Andersen et al. \cite{andersen2007local} and subsequently used as a primitive in \cite{banerjee2015fast, lee2014asynchronous}.
The critical associated invariant in Lemma \ref{lem:pushinvariant} was first shown for Personalized PageRank\cite{andersen2007local}; however, it holds more generally for any matrix $Q$ (as we show below; also see~\cite{lee2014asynchronous}).
 

For each step-index $k \in\{0,1,\ldots,\ell \}$, we store two vectors: the \emph{estimate vector} $\qest{k}$ and the \emph{residual vector} $\res{k}$.
We initialize all $\res{k},\qest{k}, k\in[\ell]$ to $0$ except for $\res{0}$, which we set to $\mathbf{e}_t$.
Now, given any dimension-index $v\in[n]$ and step-index $k \in[\ell]$, the \texttt{REVERSE-PUSH} operation iteratively updates these vectors as follows:
\begin{algorithm}[!ht]
\caption{\texttt{REVERSE-PUSH}$(t,v,k)$}
\label{alg:push}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, estimates $\qest{k}$, residuals $\res{k},\res{k+1}$
\RETURN New estimates $\qestnew{k}$, residuals $\resnew{k}$ computed as:
\begin{align*}
	\qestnew{k} &\leftarrow \qest{k} + \ip{\res{k}}{\mathbf{e}_v}\mathbf{e}_v \\
	\resnew{k} &\leftarrow \res{k} - \ip{\res{k}}{\mathbf{e}_v}\mathbf{e}_v \\
	\resnew{k+1} &\leftarrow \res{k+1} + \ip{\res{k}}{\mathbf{e}_v}(Q\mathbf{e}_v)
\end{align*}	
\end{algorithmic}
\end{algorithm}    

%The \texttt{REVERSE-PUSH} iteration results in the following critical invariant for the estimate and residual vectors:
\begin{lemma}
\label{lem:pushinvariant}
\emph{Given the initialization described above, after any sequence of \texttt{REVERSE-PUSH} operations, and for any $\bz\in\setR^n$ and $\ell\geq 0$, the estimates $\{\mathbf{q}_t^k\}$ and residuals $\{\mathbf{r}_t^k\}$  satisfy the following invariant:
\begin{align*}
\mathbf{p}_\bz^{\ell}[t] &= \left< \bz, \mathbf{q}_t^{\ell} \right> + \sum_{k=0}^{\ell} \left<\bz, Q^k\mathbf{r}_t^{\ell-k}\right> = \left< \bz, \mathbf{q}_t^{\ell}  + \sum_{k=0}^{\ell} Q^k\mathbf{r}_t^{\ell-k}\right>
\end{align*}
}
\end{lemma}

The above invariant was first stated in~\cite{andersen2007local} for the case of PageRank vectors. 
For the sake of completeness, we present a proof (adapted from~\cite{Lofgren2014}) for general matrices; an identical invariant was also presented in~\cite{lee2014asynchronous}.

\begin{proof}
For our chosen initialization (i.e., $\res{0} = \mathbf{e}_t$, and all other estimate and residual vectors set to $0$), the invariant simplifies to $\mathbf{p}_\bz^{\ell}[t] = \ip{\bz }{Q^{\ell}\mathbf{e}_t}$ which is true by definition.
Now, assuming the invariant holds at any stage with vectors $\{\qest{k}\}, \{\res{k}\}_{k\in[\ell]}$, let $\{\qestnew{k}\}, \{\resnew{k}\}_{k\in[\ell]}$ be the new vectors after executing a \texttt{REVERSE-PUSH}$(t,v,k)$ operation for any given $k\in[\ell]$ and $\bz\in\setR^n$. We define:
$$\Delta_v^{k} = \left( \mathbf{\tilde{q}}_t^{\ell} + \sum_{i=0}^{\ell}  (Q^i)\resnew{\ell-i} \right)- \left( \qest{\ell} + \sum_{i=0}^{\ell}  (Q^i) \mathbf{r}_t^{\ell-i}\right)$$
Now to show that the invariant holds following \texttt{REVERSE-PUSH}$(t,v,k)$, it suffices to show that $\Delta_v^{k}$ is zero for any $v\in V$ and $k\in[\ell]$. 

We now have three cases: $(i)$ if $\ell < k$, then the \texttt{REVERSE-PUSH}$(t,v,k)$ operation does not affect the residual or estimate vectors $\{\mathbf{q}_t^{i},\mathbf{r}_t^{i}\}_{i<k}$, and hence 
$\Delta_v^k=0$;
$(ii)$ If $\ell = k$, we have:
\begin{align*}
\Delta_v^{k} &= \left( \mathbf{\tilde{q}}_t^{k} +  \mathbf{\tilde{r}}_t^{k}\right)- \left( \mathbf{q}_t^{k} + \mathbf{r}_t^{k}\right)\\
&= \mathbf{q}_t^{k} + \left<\mathbf{r}_t^{k}, \mathbf{e}_v \right>\mathbf{e}_v +  \mathbf{r}_t^{k} - \left<\mathbf{r}_t^{k}, \mathbf{e}_v \right>\mathbf{e}_v - \mathbf{q}_t^{k} - \mathbf{r}_t^{k}
= 0
\end{align*}
$(iii)$ Finally, when $\ell > k$, we have: 
\begin{align*}
\Delta_v^k &= Q^{\ell-k}\left( \mathbf{\tilde{r}}_t^k - \mathbf{r}_t^k \right) + Q^{\ell-k-1}\left( \mathbf{\tilde{r}}_t^{k+1} - \mathbf{r}_t^{k+1} \right)\\
&= -\left<\mathbf{r}_t^k, \mathbf{e}_v \right>Q^{\ell-k}\mathbf{e}_v  + \left<\mathbf{r}_t^k, \mathbf{e}_v \right>Q^{\ell-k-1}\left(Q\mathbf{e}_v \right)
%&= -\left<\mathbf{r}_t^k, \mathbf{e}_v \right>\mathbf{e}_v(Q^T)^{\ell-k} + \left<\mathbf{r}_t^k, \mathbf{e}_v \right>\mathbf{e}_v (Q^T)^{\ell-k} 
= 0
\end{align*}
Hence we have shown that the invariant is preserved for any sequence of reverse push operations.
\end{proof}

The above invariant gives a natural iterative algorithm for computing $\pzlt$: perform repeated \texttt{REVERSE-PUSH} operations controlling the residual vectors $\mathbf{r}_t^k$, and use $\pzltest = \langle\bz,\mathbf{q}_t^{\ell}\rangle$ as the estimate. 
Depending on the norm we choose to control, we can get a bound for the error via H{\"o}lder's inequality.
In particular, placing an upper bound on the infinity norm (i.e., the maximum absolute value of the residual vectors) of some chosen $\delta_r>0$ gives us the error bounds: $|\pzlt - \langle\bz,\mathbf{q}_t^{\ell}\rangle|\leq ||x||_1\delta_r||Q||_{\infty}^{\ell}$~\footnote{This approach was used in~\cite{andersen2007local,Lofgren2014,banerjee2015fast}; an alternative is to control $||r_t^k||_2$ giving error bounds in terms of $||x||_2$, which was suggested in ~\cite{lee2014asynchronous}.}.

\begin{algorithm}[!ht]
\caption{\texttt{REVERSE-LOCAL-UPDATE}$(t,Q, \ell, \delta_r)$}
\label{alg:rwork}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, maximum step-index $\ell$, target residual threshold $\delta_r$
\STATE{Initialize all residual $\res{k}$ and estimate vectors $\qest{k}, k \in[\ell]$ to $0$; set $\res{0} = \mathbf{e}_t$}
\FOR{ $k \in \{0, 1, 2, ... \ell\}$ }
\WHILE{$\exists v$ such that $\left|\res{k}[v]\right| > \delta_r $}
\STATE{\texttt{REVERSE-PUSH}$(t,v,k)$}
\ENDWHILE
\ENDFOR
\RETURN $\{\qest{k}\}, \{\res{k}\}_{k \in[\ell]}$
\end{algorithmic}
\end{algorithm}    

Finally, we want to bound the running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\ell,\delta_r)$. 
It is easy to see that in the worst case, the running time can be as much as the $\ell$-hop in-neighborhood of $t$ in $Q$. 
However, for a \emph{uniform random} choice of $t$, we can obtain a more informative bound. 
%Recall we define $\beta =  ||Q||_{\infty}$. Now we have the following:

\begin{lemma}
\label{lem:pushruntime}
\emph{For any $Q\in\setR^{n\times n}$ and uniform random dimension-index $t\in[n]$, the expected running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\ell,\delta_r)$ is 
$$O\left(\frac{nnz(Q)}{n\delta_r}(\ell+1)||Q||_{\infty}^{\ell}\right)$$ 
}
\end{lemma}

In particular, note that if $||Q||_{\infty}\leq 1$ and $\ell=O(1)$, then the average running time is $O\left(\frac{nnz(Q)}{n\delta_r}\right)$.

\begin{proof}
Let $T(t)$ be the running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\ell,\delta_r)$.
Recall we define $Q^+$ as the matrix with $Q^+_{ij} = |Q_{ij}|$; let $\hat{T}(t)$ be the running time of \texttt{REVERSE-LOCAL-UPDATE}$(t,Q^+,\ell,\delta_r)$. 
Then we have that for every matrix $Q$ and every $t$, we have $\hat{T}(t)>T(t)$ -- this follows from the fact that any cancellation between positive and negative residuals in \texttt{REVERSE-LOCAL-UPDATE}$(t,Q^+,\ell,\delta_r)$ can only decrease the number of iterations. 
Also, note that under $Q^+$ all residuals are positive, so we have for any $k\leq\ell,v\in[n]$, the residuals satisfy $\mathbf{r}_t^{k}[v]\leq \left(\mathbf{e}_v^TQ^\ell\right)[t]$.

Now let $d_i := \sum_j\mathds{1}_{\{Q_{ij}\neq 0\}}$, i.e., the support of $i^{th}$ row in $Q$, and $\mathbf{r}_t^{k}$ denote the residuals under \texttt{REVERSE-LOCAL-UPDATE}$(t,Q^+,\ell,\delta_r)$. 
%Recall we define $\beta = \max\{1,||Q||_{\infty}\}$.
From Algorithm \ref{alg:rwork}, we have $\hat{T}(t) = \sum_{k=0}^{\ell}\sum_{v\in[n]}\mathds{1}_{\mathbf{r}_t^{k}[v]>\delta_r}$.
Thus, the expected running time over a uniform random choice of $t\in[n]$ is given by
\begin{align*}
\frac{1}{n}\sum_t\hat{T}(t) &= \frac{1}{n}\sum_{t\in[n]}\sum_{k=0}^{\ell}\sum_{v\in[n]}\mathds{1}_{\{\mathbf{r}_t^{k}>\delta_r\}}d_v\\
&=\frac{1}{n}\sum_{k=0}^{\ell}\sum_{v\in[n]}\sum_{t\in[n]}\mathds{1}_{\{\mathbf{r}_t^{k}>\delta_r\}}d_v\\
&\leq\frac{1}{n}\sum_{k=0}^{\ell}\sum_{v\in[n]} \mathds{1}_{\{\left(\mathbf{e}_w^T(Q^+)^k\right)[t]>\delta_r\}}d_v\\
&=\frac{1}{n}\sum_{k=0}^{\ell}\sum_{v\in[n]} \frac{||\mathbf{e}_w^T(Q^+)^k||_1}{\delta_r}d_v\\
&\leq\frac{1}{n}\sum_{k=0}^{\ell}\sum_{v\in[n]} \frac{||Q^+||_{\infty}^k}{\delta_r}d_v\\
&\leq\left(\ell+1\right)||Q||_{\infty}^{\ell} \frac{nnz(Q)}{n\delta_r}
\end{align*}
\end{proof}


\subsection{Computing Matrix Powers via MCMC Sampling}
\label{ssec:forwardwork}


In the previous section, we computed $\pzlt$ by working backwards from $t$. 
Note that our final algorithm is independent of $\bz$.
We now present an alternate technique which is based on a forward MCMC sampling technique called the von Neumann-Ulam scheme. 
In this case, the algorithm starts from $\bz$, and computes $\pzlt$ for all $t\in[n]$.

More generally, given any vectors $\mathbf{a}$ and $\mathbf{b}$ and matrix $Q$, the von Neumann-Ulam scheme can be used for computing $\ip{\mathbf{a}}{Q^{\ell}\mathbf{b}}$.
To understand the algorithm, note that we can expand $\ip{\mathbf{a}}{Q^{\ell}\mathbf{b}}$ as the sum $\sum_{(v_0,\ldots, v_{\ell}) \in V^{\ell}}\left(\prod_{j \in [{\ell}]} Q_{v_{j-1}v_{j}} \right) \mathbf{a}[v_0] \mathbf{b}[v_{\ell}]$. 
Now we can interpret this sum as an expectation over an ${\ell}$-step random walk $W = (V_0,V_1,\ldots,V_{\ell})$ on $\G$, specified as follows:
\begin{itemize}
\item $V_0$, the starting node for the random walk, is sampled from $\sigma_{\mathbf{a}} = \{|\mathbf{a}[i]|/||\mathbf{a}||_1\}_{i\in [n]}$, and has an associated weight $w_{V_0} = sgn(\mathbf{a}[V_0])||\mathbf{a}||_1$.
\item The transition probability matrix for the walk is given by $P_{ij} = \{|Q_{ij}|/||Q_i||_1\}$ (where $||Q_i||_1$ is the $1$-norm of the $i^{th}$ row of $Q$).
\item Each edge $(i,j)\in E$ has associated weight $w_{ij} = sgn(Q_{ij})||Q_i||_1$. 
\item The `score' for a walk $W$ is the product of weights of traversed edges, i.e.,
$$S_t^{\ell}(W) = w_{V_0}\prod_{i=0}^{{\ell}-1}w_{V_iV_{i+1}}\mathbf{b}[V_{\ell}]$$
\end{itemize}
%We can then sample paths according to $P$ and weight each of these products by the normalization factors we used to construct $P$ and $\sigma$ from $Q$ and $\bz$. There are two aspects to normalizing $Q$ and $\bz$, the sign, and ensuring row sums are equal to $1$. By normalizing the substochastic matrix $Q$ to get the stochastic $P$, we are removing a dummy state $A$ that absorbed probability for $Q$. 
%We construct $\mathbf{p}_A$ to store this leftover probability mass. 
%We sample from the distribution $\sigma (P)^k$ and then weight by the normalization factors to compute $\sum_{(v_0,\ldots, v_k) \in V^k}\left(\prod_{j \in [k]} Q_{v_{j-1}v_{j}} \right) \bz[v_0] \mathbf{r}_t^{\ell-k}[v_k]$ as an expectation. 
This procedure is summarized in Algorithm \ref{alg:fwalk}. 

\begin{algorithm}[ht]
\caption{\texttt{MCMC-SAMPLER}$(Q, {\ell}, \mathbf{a}, \mathbf{b})$}
\label{alg:fwalk}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, exponent ${\ell}$, vectors $\mathbf{a}$ and $\mathbf{b}$
\STATE{Construct transition matrix $P_{ij} = \{|Q_{ij}|/||Q_i||_1\}$ and starting measure $\sigma_a = \{|\mathbf{a}[i]|/||\mathbf{a}||_1\}_{i\in V}$}
\STATE{Define node weights $w_{V_0} = sgn(\mathbf{a}[V_0])||\mathbf{a}||_1$ and edge weights $w_{V^iV^j} = sgn(Q_{V^iV^j})||Q_i||_1$}
\STATE{Construct source distribution $\sigma_{\mathbf{a}}$ with $\sigma_{\mathbf{a}}[i] = \frac{\bz[i]}{||\bz||_1}$}
\STATE{Sample $V^0 \sim \sigma_a$, and generate a random walk $W = \{V^0, V^1,\ldots, V^{\ell}\}$ of length ${\ell}$ on $\G$  using transition probability $P$.}
\RETURN Walk-score $S_{t}^{\ell}(W) = w_{V_0}\prod_{i=0}^{{\ell}-1}w_{V_iV_{i+1}}\mathbf{b}[V_{\ell}]$
\end{algorithmic}
\end{algorithm} 

%Once we have the sampled scores for $n_f$ walks each of length $l$, we can combine the estimate $\ip{\bz}{\qest{\ell}}$ with an average over the scores $\{S_{t, i}^{\ell}\}$ from to compute the overall estimator $\pzltest$.
%Again, as noted in the section describing the forward walk estimation, provided that $Q$ is substochastic with spectral norm less than $1$, we can find an induced transition matrix that is fully stochastic as well as the described normalization terms $\pabsorb{\ell}$.

%Recall we define $\beta = ||Q||_{\infty}$. 
Now we have the following Lemma.
\begin{lemma}
\label{lem:mcmc}	
\emph{
\texttt{MCMC-SAMPLER}$(Q, {\ell},\mathbf{a},\mathbf{b})$ returns a walk-score $S_{t}^{\ell}(W)$ which satisfies:
\begin{enumerate}
\item $\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^{\ell}} \left[S_{t}^{\ell}(W)\right] = \left<\mathbf{a}, Q^{\ell}\mathbf{b} \right>$
\item $S_{t}^{\ell}(W) \in [\;-||Q||_{\infty}^{\ell}||\mathbf{a}||_1||\mathbf{b}||_{\infty} ,\; ||Q||_{\infty}^{\ell}||\mathbf{a}||_1||\mathbf{b}||_{\infty} \;]$
\end{enumerate}
}
\end{lemma}


\begin{proof}
We can expand $\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^{\ell}} \left[S_{t}^{\ell}(W)\right]$ to obtain:
\small
\begin{align*}
&\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^{\ell}} \left[S_{t}^{\ell}(W)\right] \\ 
&= \sum_{(V^0, ... V^{\ell}) \in [n]^{\ell+1}} \left(\frac{sgn(\mathbf{a}[V_0])||\mathbf{a}||_1|\mathbf{a}[V^0]|}{||\mathbf{a}||_1} \right. \\
&\left. \times \left( \prod_{j \in [0, \ell-1]} \frac{sgn(Q_{V^jV^{j+1}}||Q_{V^j}||_1)|Q_{V^jV^{j+1}}|}{||Q_{V^j}||_1} \right) \mathbf{b}[V^{\ell}] \right) \\
& = \sum_{(V^0, ... V^{k}) \in [n]^{\ell+1}} \left(\mathbf{a}[V_0] \left( \prod_{j \in [0,\ell-1]} Q_{V^jV^{j+1}}\right) \mathbf{b}[V^{\ell}] \right)
\end{align*}
\normalsize
The final expression is exactly the definition of $\ip{\mathbf{a}}{Q^{\ell}\mathbf{b}}$; hence $\mathbb{E}_{W \sim \sigma_{\mathbf{a}}(P)^{\ell}} \left[S_{t}^{\ell}(W)\right] = \ip{\mathbf{a}}{Q^{\ell}\mathbf{b}}$.


Next, in order to bound the score $S_{t}^{\ell}(W)$, recall that
\begin{align*}
S_{t}^{\ell}(W) = w_{V_0}\prod_{i=0}^{\ell-1}w_{V_iV_{i+1}}\mathbf{b}[V_{\ell}]
\end{align*}

We defined $w_{V_0} = sgn(\mathbf{a}[V_0])||\mathbf{a}||_1$, so trivially $|w_{V_0}| \leq ||\mathbf{a}||_1$.
Additionally, $w_{V_iV_{i+1}} = sgn(Q_{V^iV^j})||Q_i||_1 $ and by definition $||Q_i||_1  \leq ||Q||_{\infty}$ and $| \mathbf{b}[V^{\ell}] |\leq ||\mathbf{b}||_\infty$.
Hence, since we are multiplying ${\ell}$ edge scores, we finally obtain
$|S_{t}^{\ell}(W)| \leq ||Q||_{\infty}^{\ell}||\mathbf{a}||_1||\mathbf{b}||_\infty$ as stated in the lemma.
\end{proof}

