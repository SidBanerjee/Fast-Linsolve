%!TEX root = main.tex

\section{A Bidirectional Algorithm for Computing Matrix Powers}
\label{ssec:bidiralgo}

Finally, we present our main contribution: a bidirectional estimator for $\pzlt = \langle\bz,Q^{\ell}\be_t\rangle$. 
Our algorithm follows the general structure proposed by Lofgren et al.~\cite{Lofgren2014,banerjee2015fast} for PageRank and Markov Chain transition probability estimation.
It comprises of two distinct components: first we use  \texttt{REVERSE-LOCAL-UPDATE} to estimate approximate values of $\left(Q^{\ell}\be_t\right)[i]$ for all steps $\ell \in[\lm]$ and $i\in[n]$. 
We then use \texttt{MCMC-SAMPLER} to reduce the error in these estimates to get our desired accuracy.


Intuitively, the advantage we gain from combining the two previous algorithms is that running a small amount of local-updates reduces the variance in the walk-scores significantly, which then allows us to perform sampling more effectively.
More specifically, given a desired error threshold $\delta$, we first run \texttt{REVERSE-LOCAL-UPDATE}$(t,Q,\ell,\delta_r)$ for an appropriately chosen $\delta_r\gg\delta$ (cf. Theorem \ref{thm:main}). 
At this point, from Lemma \ref{lem:pushinvariant}, we know that we have
$\pzlt = \ip{\bz}{\mathbf{q}_t^{\ell}} + \sum_{k=0}^{\ell} \ip{\bz}{Q^k\mathbf{r}_t^{\ell-k}}$, with $\mathbf{r}_t^{k}[v]\leq\delta_r$ for all $v\in[n],k\leq\ell$.
Now, instead of ignoring the residual terms (as done in~\cite{andersen2007local,lee2014asynchronous}; cf. Section \ref{ssec:reverse}), we can further reduce our error by using \texttt{MCMC-SAMPLER}$(Q,k,\bz,\mathbf{r}_t^k)$ to estimate the residual $\langle\bz,\mathbf{r}_t^k\rangle$ for all $k\leq\ell$.
The resulting error is better than ignoring the residual, and also better than directly executing \texttt{MCMC-SAMPLER}$(Q,\ell,\bz,\be_{t})$, as the residuals have magnitude much smaller than $1$, and hence the walk-scores have lower variance.
%\begin{algorithm}[ht]
%\caption{\texttt{BIDIR-MATRIX-POWER}$(Q, \bz, t,\lm)$}
%\label{alg:linearsysest}
%\begin{algorithmic}[1]
%\REQUIRE Matrix $Q$, vector $\bz$, target node-index $t$.
%\STATE{Compute estimate and residual vectors via $\{\res{\ell}\}, \{\qest{\ell}\}$ = \texttt{REVERSE-LOCAL-UPDATE}$(t,Q, \lm, \delta_r)$}
%\FOR{ $l \in \{1, 2,\ldots, \lm \}$}
%\FOR{ $i \in [n_f]$}
%\STATE{$k \sim Unif[0, \ell]$}
%\STATE{$S_{i,t}^\ell$ = \texttt{MCMC-SAMPLER} $(Q, k, \bz, \ell \cdot \res{\ell - k})$}
%\ENDFOR
%\STATE{$\pzlt = \ip{\bz}{\qest{\ell}} + \frac{1}{n_f} \sum_{i=0}^{n_f}S_{i,t}^\ell$}
%\ENDFOR
%\RETURN $\sum_{\ell=0}^{\lm} \pzlt$ 
%\end{algorithmic}
%\end{algorithm} 

\begin{algorithm}[ht]
\caption{\texttt{BIDIR-MATRIX-POWER}$(Q, \bz, t,\ell)$}
\label{alg:linearsysest}
\begin{algorithmic}[1]
\REQUIRE Matrix $Q$, exponent $\ell$, vector $\bz$, target index $t$.
\STATE{Compute estimate and residual vectors $\{\res{k}, \qest{k}\}_{\{k\leq\ell\}}$ = \texttt{REVERSE-LOCAL-UPDATE}$(t,Q, \lm, \delta_r)$}
%\FOR{ $l \in \{1, 2,\ldots, \lm \}$}
\FOR{ $i \in [n_f]$}
\STATE{$k \sim Unif[0, \ell]$}
\STATE{$S_{i,t}^\ell$ = \texttt{MCMC-SAMPLER} $(Q, k, \bz, \res{\ell - k})$}
\ENDFOR
\RETURN $\pzltest = \ip{\bz}{\qest{\ell}} + \frac{\ell+1}{n_f} \sum_{i=1}^{n_f}S_{i,t}^\ell$
%\ENDFOR
%\RETURN $\sum_{\ell=0}^{\lm} \pzlt$ 
\end{algorithmic}
\end{algorithm} 

First, we observe that \texttt{BIDIR-MATRIX-POWER} always returns an unbiased estimate for $\pzlt$ for any choice of $n_f$.

\begin{lemma}
\label{lem:unbiased}
\emph{
\texttt{BIDIR-MATRIX-POWER} returns an unbiased estimator of $\pzlt$, i.e. $\mathbb{E}[\pzltest] = \pzlt$
%\begin{align*}
%\EE[pzltest] = \pzlt 
%&=  \left<\bz, Q^l \mathbf{e}_t \right>= \left<\bz, \mathbf{q}_t^{\ell} \right> + \sum_{k=0}^{\ell} \left<\bz , Q^k\mathbf{r}_t^{\ell-k}\right>= \mathbb{E}\left[\hat{\mathbf{p}}_\bz^{\ell}[t] \right]
%\end{align*}
}
\end{lemma}

\begin{proof}
By definition of $\pzltest$ and linearity of expectation
\begin{align*}
\mathbb{E}\left[\pzltest\right] 
%&= \left<\bz, \mathbf{q}_t^{\ell} \right> + \frac{1}{n_f}\sum_{i=1}^{n_f} \mathbb{E}_{k \sim Unif[0, l]}\left[S_{t}^{k}\right] \\
&= \left<\bz, \mathbf{q}_t^{\ell} \right> + (\ell+1)\mathbb{E}_{K \sim Unif[0, \ell]}\left[S_{t}^{K}\right]
\end{align*}

From Lemma \ref{lem:mcmc}, we know \texttt{MCMC-SAMPLER} $(Q, k, \bz, \res{k})$ returns an estimate satisfying $\mathbb{E}\left[S_{t}^{k}\right] = \ip{\bz}{Q^k \res{k}}$ for all $k\leq \ell$. Thus, with $K \sim Unif[0, \ell]$, we have
\begin{align*}
\mathbb{E}\left[\pzltest \right] &= \left<\bz, \mathbf{q}_t^{\ell} \right> + (\ell+1)\mathbb{E}_K\left[\ip{\bz}{Q^K \mathbf{r}_t^{\ell-K}} \right] \\
&= \left<\bz, \mathbf{q}_t^{\ell} \right> + \sum_{k = 0}^\ell\ip{\bz}{Q^k \mathbf{r}_t^{\ell-k}}
\end{align*}
However, Lemma \ref{lem:pushinvariant} states that after any sequence of reverse push operations, we have the invariant $\pzlt = \left<\bz, \mathbf{q}_t^{\ell} \right> + \sum_{k=0}^{\ell} \left<\bz, Q^k\mathbf{r}_t^{\ell-k}\right>$. Thus, $\mathbb{E}\left[\pzltest \right] = \pzlt$.
\end{proof}

To choose $n_f$ to get the desired accuracy, we need the following concentration bound, which is a restatement of Hoeffding's inequality (cf. Chapter $1$ in~\cite{dubhashi2009concentration}).
\begin{lemma}[Hoeffding's Inequality]
\label{lem:hoeff}
\emph{
Given $\{X_i\}$ independent random variables s.t. for all $i$, $X_i \in [-c, c]$ a.s., and $|\mathbb{E}[X_i]| \geq a$. 
Then $X = \sum_{i= 1}^n X_i$ satisfies
$\mathbb{P}[|X - \mathbb{E}[X]| \geq \epsilon \mathbb{E}[X] ] \leq p_{fail}$ provided that
$$n \geq \frac{2c^2}{\epsilon^2 a^2}\ln\left(\frac{2}{p_{fail}} \right)$$
}
\end{lemma}

\begin{comment}
\begin{proof}
Let $X = \sum_{i=1}^n X_i$.  Since $E[X] = nE[X_i]$, $E[X] \geq n \epsilon a$. Let $t = \epsilon a$.
\begin{align*}
\mathbb{P}\left[|X - \mathbb{E}[X]| \geq \epsilon \mathbb{E}[X] \right] 
&\leq \mathbb{P}\left[|X - \mathbb{E}[X]| \geq nt \right] \\
&= \mathbb{P}\left[\left|\frac{1}{n}X - \frac{1}{n}\mathbb{E}[X]\right| \geq t \right]  
\end{align*}
Applying Hoeffding's inequality to the rightmost term above, we obtain
  \[\mathbb{P}\left[|X - \mathbb{E}[X]| \geq \epsilon \mathbb{E}[X] \right] \leq 2\exp \left(-\frac{2n^2t^2}{\sum_{i=1}^n(b_i - a_i)^2}\right) \]
  Hence substituting the values for $t$ and $b_i = c$, $a_i = -c$ gives us:
\[\mathbb{P}\left[|X - \mathbb{E}[X]| \geq \epsilon \mathbb{E}[X] \right] \leq 2\exp \left(-\frac{2n^2\epsilon^2a^2}{n(4c^2)}\right) \]
Now set this upperbound to $p_{fail}$ to obtain the relation:
$$ \frac{n \epsilon^2 a^2}{2c^2} = \ln\left(\frac{2}{p_{fail}} \right) $$
Thus, for $n \geq \frac{2c^2}{\epsilon^2 a^2}\ln\left(\frac{2}{p_{fail}} \right)$, we are guaranteed that $\mathbb{P}\left[|X - \mathbb{E}[X]| \geq \epsilon \mathbb{E}[X] \right] \leq p_{fail}$.

Note that if we require the failure conditions to hold for $\lm$ sets of $X's$, then we perform a simple union bound which adds a $\lm$ in the $\frac{2}{p_{fail}}$ term.
\end{proof} 
\end{comment}


Now we can state our main result, where we show how to choose $\delta_r$ and $n_f$ such that \texttt{BIDIR-MATRIX-POWER} returns an estimate of desired accuracy. 
The choices of $(n_f,\delta_r)$ also affect the running time guarantee, which, as in Lemma \ref{lem:pushruntime}, is averaged over all targets $t\in[n]$. 
To this end, we define $T(t)$ to be the running time for a target index $t$. 

Finally, we have our main theorem:
\begin{theorem}
\label{thm:main}
\emph{Given matrix $Q$ and vector $\bz$, let $\pzltest$ be the estimate of $\pzlt$ returned by the \texttt{BIDIR-MATRIX-POWER} algorithm with 
\begin{align*}
\delta_r &= \sqrt[3]{\frac{nnz(Q) \epsilon^2\delta^2}{\ell^2 n||\bz||_1^2||Q||_{\infty}^{\ell}\ln(\ell/p_{fail}) } }\\
n_f &= \frac{2 \lm^2|\bz|_1^2 \delta_r^2 ||Q||_{\infty}^{2\lm}}{\epsilon^2\delta^2}\ln \left(\frac{2}{p_{fail}}\right)
\end{align*}
Then we have the following
\begin{itemize}
\item For all $t\in[n]$, we have $|\pzltest-\pzlt|\leq \max\{\delta,\epsilon\pzlt\}$ with probability at least $1-p_{fail}$ 
\item For a uniform random choice of $t\in[n]$, the expected running time of the algorithm is
$$
\mathbb{E}[T(t)] = O\left( \left(\frac{||\bz||_1 nnz(Q)}{\epsilon \delta n} \right)^{2/3} \ell^{5/3}\left(\ln \frac{2}{p_{fail}} \right)^{1/3} \right)$$
\end{itemize}
}
\end{theorem}

In particular, suppose we choose $\delta = 1/n$, and assuming $||\bz||_1\leq 1$ and $||Q||_{\infty}\leq 1$, and $\ell,1/p_{fail}=O(1)$, then we get $\mathbb{E}[T(t)] = O\left(\left(nnz(Q)/\epsilon\right)^{2/3}\right)$, as stated in Section \ref{sec:intro}.

\begin{proof}
%We will prove this statement by first considering the single estimator $\pzltest$ for some $\ell$ since $\ip{\mathbf{x}_t}{\mathbf{e}_t} = \sum_{l=0}^{\infty} \pzlt$.
Consider the random walk-score $\mathcal{S}_{t}^{\ell} = ((\ell+1)/n_f)\sum_{i=1}^{n_f}S_{i,t}^{\ell}$ as in Algorithm \ref{alg:linearsysest}. 
From Lemma \ref{lem:unbiased} we have that $\EE\left[\mathcal{S}_{t}^{\ell}\right] = \pzlt - \ip{\sigma}{\qest{\ell}}$; moreover, from Lemma \ref{lem:mcmc}, we have that $|\mathcal{S}_{t}^{\ell}|\leq (\ell+1)||\bz||_1\delta_r||Q||_{\infty}^{\ell}$.
Thus, to achieve relative error $\epsilon$ with probability at least $1-p_{fail}$, we have from Lemma \ref{lem:hoeff} that we need
\begin{align*}
n_f\geq \frac{2 (\ell+1)^2|\bz|_1^2 \delta_r^2 ||Q||_{\infty}^{2\lm}}{\epsilon^2\delta^2}\ln \left(\frac{2}{p_{fail}}\right)
\end{align*}
Given this choice, we know that the running time for the forward MCMC sampling is $O(n_f\ell)$; moreover, from Lemma \ref{lem:pushruntime}, we get that the reverse work running time for a uniform random choice of $t\in[n]$ is $\frac{nnz(Q)}{n\delta_r}(\ell+1)||Q||_{\infty}^{\ell}$. 
Now substituting the value of $\delta_r$ given in the statement (which is chosen to balance the two running times), we get the promised running time guarantee.
%\[\frac{nnz(Q)}{n\delta_r}(\ell+1)\beta^{\ell} = \frac{(\ell+1)^2 \delta_r^2||\bz||_1^2 \beta^{2\ell}}{\epsilon^2\delta^2}\ln \left(\frac{\ell+1}{p_{fail}}\right) \]
%Thus we obtain
%\begin{equation*}
%\delta_r = \sqrt[3]{\frac{nnz(Q) \epsilon^2\delta^2}{\ell^2 n||\bz||_1^2\beta^{\ell}\ln(\ell/p_{fail}) } }
%\end{equation*}
%$$\mathbb{E}[T(t)] = O\left( \left(\frac{\lm^2 |\bz|_1 nnz(Q)}{\epsilon \delta n} \right)^{2/3} \left(\ln \frac{\lm}{p_{fail}} \right)^{1/3} \right)$$
%which is in fact the total running time.
\end{proof}
%
%
%\begin{lemma}
%Set $\lm = \frac{1}{\ln \rho(G)} \ln \left(\frac{\delta(1 - \rho(G))}{||\bz||} \right)$  to satisfy additive error threshold of $\delta$.
%\end{lemma}
%\begin{proof}
%Note that $\lm$ controls error by truncating the power series $\sum_{l=0}^\infty \ip{\bz}{Q^l\mathbf{e}_t}$.
%Suppose we have this error as $\Delta =\sum_{l=0}^\infty \ip{\bz}{Q^l\mathbf{e}_t} - \sum_{l=0}^{\lm} \ip{\bz}{Q^l\mathbf{e}_t} = \ip{\bz}{Q^{\lm} \sum_{l=1}^\infty Q^{\ell} \mathbf{e}_t}$.
%Then $\Delta(\lm) \leq ||\bz|| \frac{\rho(G)^{\lm}}{1 - \rho(G)}$.
%If we want additive error $\delta$ (that is, $ \delta \leq \Delta(\lm)$), provided $\delta \leq ||\bz||$, we have
%$\lm \geq \frac{1}{\ln \rho(G)} \ln \left(\frac{\delta(1 - \rho(G))}{||\bz||} \right)$.
%Recall that $\rho(G) < 1$, so $\ln \rho(G) < 0$ and the suggested value for $\lm$ increases as $\delta$ shrinks.
%\end{proof}
